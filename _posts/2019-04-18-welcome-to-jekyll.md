Inoltre, il libro presenta una prospettiva bayesiana, che è diventata popolare nel campo dell'apprendimento automatico negli ultimi anni. Il libro spiega anche algoritmi di inferenza approssimata che permettono risposte rapide in situazioni in cui quelle esatte non sono fattibili.

È importante sottolineare anche cosa manca e, principalmente, si tratta di tutto ciò che rientra nel novero dell'apprendimento profondo, che utilizza reti neurali con molteplici livelli di elaborazione per estrarre caratteristiche di livello superiore dai dati grezzi. Alcuni degli algoritmi e delle architetture più importanti in questo campo includono:

- Reti neurali convoluzionali (_convolutional neural networks_, CNNs):
	- Utilizzate principalmente per compiti di visione artificiale, come il riconoscimento di immagini e video.
	- Esempi: LeNet, AlexNet, VGGNet, ResNet, GoogleNet/inception, MobileNets.
- Reti neurali ricorrenti (_recurrent neural networks_, RNNs):
	- Progettate per lavorare con sequenze di dati, come serie temporali o linguaggio naturale.
	- Esempi: LSTM (_long short-term memory_), GRU (_gated recurrent units_).
- Codificatore automatico (_autoencoder_):
	- Utilizzati per l'apprendimento non supervisionato di codifiche efficienti, riduzione della dimensionalità e apprendimento di rappresentazioni.
	- Esempi: Autoencoder sparsi, Denoising Autoencoder, Variational Autoencoder (VAE).
- Reti neurali generative avversarie (_generative adversarial networks_, GANs):
	- Composte da due reti neurali, il generatore e il discriminatore, che vengono addestrate simultaneamente attraverso un gioco avversario. Utilizzate per generare dati che assomigliano a dati veri, come immagini, video, e musica.
- Reti neurali di capsule (_capsule neural network_, CapsNet):
	- Un tentativo di migliorare le CNN utilizzando "capsule" che conservano le informazioni spaziali e migliorano la capacità della rete di riconoscere oggetti da diverse angolazioni e posizioni.
- Reti neurali con memoria esterna (_memory-augmented neural networks_, MANNs):
	- Combinano RNN con una memoria esterna che la rete può leggere e scrivere, permettendo alla rete di compiere compiti che richiedono comprensione e ragionamento.
	- Esempi: NTM (_neural Turing machines_), DNC (_differentiable neural computers_).
- Trasformatori (_transformer_):
	- Un tipo di architettura che utilizza meccanismi di attenzione per migliorare il trattamento delle sequenze di dati, particolarmente influente nel campo del processamento del linguaggio naturale.
	- Esempi: BERT, GPT (_Generative Pre-trained Transformer_), T5, XLNet.